<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>sequence_transformer.training_utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sequence_transformer.training_utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow as tf
import math
from sequence_transformer.constants import LABEL_PAD


class CustomLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000, scale=1):
        super(CustomLRSchedule, self).__init__()

        self.d_model = float(d_model)  # Don&#39;t tf.cast this. It will result in: &#34;Tensor object not json serializable&#34;
        self.warmup_steps = warmup_steps
        self.scale = scale

    def get_config(self):
        config = {
            &#39;d_model&#39;: self.d_model,
            &#39;warmup_steps&#39;: self.warmup_steps,
            &#39;scale&#39;: self.scale
        }
        return config

    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        learning_rate = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) * self.scale

        return learning_rate * self.scale


class CustomExponentialDecayLR(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, initial_learning_rate, limiting_learning_rate, decay_steps, decay_rate):
        super(CustomExponentialDecayLR, self).__init__()

        self.initial_learning_rate = initial_learning_rate
        self.limiting_learning_rate = limiting_learning_rate
        self.decay_steps = decay_steps
        self.decay_rate = decay_rate

    def get_config(self):
        config = {
            &#39;init_lr&#39;: self.initial_learning_rate,
            &#39;limit_lr&#39;: self.limiting_learning_rate,
            &#39;decay_steps&#39;: self.decay_steps,
            &#39;decay_rate&#39;: self.decay_rate
        }
        return config

    def __call__(self, step):
        learning_rate = (self.initial_learning_rate - self.limiting_learning_rate) * tf.math.pow(self.decay_rate, (tf.math.divide(step, self.decay_steps))) + self.limiting_learning_rate
        return learning_rate


class BestModelSaverCallback(tf.keras.callbacks.Callback):
    def __init__(self, savedmodel_path):
        super(BestModelSaverCallback, self).__init__()
        self.savedmodel_path = savedmodel_path
        self.best_val_loss = math.inf

    def on_epoch_end(self, epoch, logs=None):
        if logs[&#39;val_loss&#39;] &lt; self.best_val_loss:
            # save_path = os.path.join(self.savedmodel_path, &#39;epoch_%03d&#39; % (epoch + 1))  # epoch is 0-based
            tf.saved_model.save(self.model, self.savedmodel_path, signatures={&#39;serving_default&#39;: self.model.model_server})
            self.best_val_loss = logs[&#39;val_loss&#39;]


class LRTensorBoard(tf.keras.callbacks.TensorBoard):
    def __init__(self, log_dir, **kwargs):  # add other arguments to __init__ if you need
        super().__init__(log_dir=log_dir, **kwargs)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs.update({&#39;lr&#39;: tf.keras.backend.eval(self.model.optimizer.lr)})
        super().on_epoch_end(epoch, logs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sequence_transformer.training_utils.BestModelSaverCallback"><code class="flex name class">
<span>class <span class="ident">BestModelSaverCallback</span></span>
<span>(</span><span>savedmodel_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class used to build new callbacks.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>params</code></strong></dt>
<dd>Dict. Training parameters
(eg. verbosity, batch size, number of epochs&hellip;).</dd>
<dt><strong><code>model</code></strong></dt>
<dd>Instance of <code>keras.models.Model</code>.
Reference of the model being trained.</dd>
</dl>
<p>The <code>logs</code> dictionary that callback methods
take as argument will contain keys for quantities relevant to
the current batch or epoch (see method-specific docstrings).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BestModelSaverCallback(tf.keras.callbacks.Callback):
    def __init__(self, savedmodel_path):
        super(BestModelSaverCallback, self).__init__()
        self.savedmodel_path = savedmodel_path
        self.best_val_loss = math.inf

    def on_epoch_end(self, epoch, logs=None):
        if logs[&#39;val_loss&#39;] &lt; self.best_val_loss:
            # save_path = os.path.join(self.savedmodel_path, &#39;epoch_%03d&#39; % (epoch + 1))  # epoch is 0-based
            tf.saved_model.save(self.model, self.savedmodel_path, signatures={&#39;serving_default&#39;: self.model.model_server})
            self.best_val_loss = logs[&#39;val_loss&#39;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.callbacks.Callback</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sequence_transformer.training_utils.BestModelSaverCallback.on_epoch_end"><code class="name flex">
<span>def <span class="ident">on_epoch_end</span></span>(<span>self, epoch, logs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<h2 id="arguments">Arguments</h2>
<p>epoch: Integer, index of epoch.
logs: Dict, metric results for this training epoch, and for the
validation epoch if validation is performed. Validation result keys
are prefixed with <code>val_</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_epoch_end(self, epoch, logs=None):
    if logs[&#39;val_loss&#39;] &lt; self.best_val_loss:
        # save_path = os.path.join(self.savedmodel_path, &#39;epoch_%03d&#39; % (epoch + 1))  # epoch is 0-based
        tf.saved_model.save(self.model, self.savedmodel_path, signatures={&#39;serving_default&#39;: self.model.model_server})
        self.best_val_loss = logs[&#39;val_loss&#39;]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="sequence_transformer.training_utils.CustomExponentialDecayLR"><code class="flex name class">
<span>class <span class="ident">CustomExponentialDecayLR</span></span>
<span>(</span><span>initial_learning_rate, limiting_learning_rate, decay_steps, decay_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>A serializable learning rate decay schedule.</p>
<p><code>LearningRateSchedule</code>s can be passed in as the learning rate of optimizers in
<code>tf.keras.optimizers</code>. They can be serialized and deserialized using
<code>tf.keras.optimizers.schedules.serialize</code> and
<code>tf.keras.optimizers.schedules.deserialize</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomExponentialDecayLR(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, initial_learning_rate, limiting_learning_rate, decay_steps, decay_rate):
        super(CustomExponentialDecayLR, self).__init__()

        self.initial_learning_rate = initial_learning_rate
        self.limiting_learning_rate = limiting_learning_rate
        self.decay_steps = decay_steps
        self.decay_rate = decay_rate

    def get_config(self):
        config = {
            &#39;init_lr&#39;: self.initial_learning_rate,
            &#39;limit_lr&#39;: self.limiting_learning_rate,
            &#39;decay_steps&#39;: self.decay_steps,
            &#39;decay_rate&#39;: self.decay_rate
        }
        return config

    def __call__(self, step):
        learning_rate = (self.initial_learning_rate - self.limiting_learning_rate) * tf.math.pow(self.decay_rate, (tf.math.divide(step, self.decay_steps))) + self.limiting_learning_rate
        return learning_rate</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sequence_transformer.training_utils.CustomExponentialDecayLR.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;init_lr&#39;: self.initial_learning_rate,
        &#39;limit_lr&#39;: self.limiting_learning_rate,
        &#39;decay_steps&#39;: self.decay_steps,
        &#39;decay_rate&#39;: self.decay_rate
    }
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="sequence_transformer.training_utils.CustomLRSchedule"><code class="flex name class">
<span>class <span class="ident">CustomLRSchedule</span></span>
<span>(</span><span>d_model, warmup_steps=4000, scale=1)</span>
</code></dt>
<dd>
<div class="desc"><p>A serializable learning rate decay schedule.</p>
<p><code>LearningRateSchedule</code>s can be passed in as the learning rate of optimizers in
<code>tf.keras.optimizers</code>. They can be serialized and deserialized using
<code>tf.keras.optimizers.schedules.serialize</code> and
<code>tf.keras.optimizers.schedules.deserialize</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000, scale=1):
        super(CustomLRSchedule, self).__init__()

        self.d_model = float(d_model)  # Don&#39;t tf.cast this. It will result in: &#34;Tensor object not json serializable&#34;
        self.warmup_steps = warmup_steps
        self.scale = scale

    def get_config(self):
        config = {
            &#39;d_model&#39;: self.d_model,
            &#39;warmup_steps&#39;: self.warmup_steps,
            &#39;scale&#39;: self.scale
        }
        return config

    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        learning_rate = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) * self.scale

        return learning_rate * self.scale</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sequence_transformer.training_utils.CustomLRSchedule.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;d_model&#39;: self.d_model,
        &#39;warmup_steps&#39;: self.warmup_steps,
        &#39;scale&#39;: self.scale
    }
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="sequence_transformer.training_utils.LRTensorBoard"><code class="flex name class">
<span>class <span class="ident">LRTensorBoard</span></span>
<span>(</span><span>log_dir, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Enable visualizations for TensorBoard.</p>
<p>TensorBoard is a visualization tool provided with TensorFlow.</p>
<p>This callback logs events for TensorBoard, including:</p>
<ul>
<li>Metrics summary plots</li>
<li>Training graph visualization</li>
<li>Activation histograms</li>
<li>Sampled profiling</li>
</ul>
<p>If you have installed TensorFlow with pip, you should be able
to launch TensorBoard from the command line:</p>
<pre><code>tensorboard --logdir=path_to_your_logs
</code></pre>
<p>You can find more information about TensorBoard
<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">here</a>.</p>
<p>Example (Basic):</p>
<pre><code class="language-python">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=&quot;./logs&quot;)
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
# run the tensorboard command to view the visualizations.
</code></pre>
<p>Example (Profile):</p>
<pre><code class="language-python"># profile a single batch, e.g. the 5th batch.
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs',
                                                      profile_batch=5)
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
# Now run the tensorboard command to view the visualizations (profile plugin).

# profile a range of batches, e.g. from 10 to 20.
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs',
                                                      profile_batch='10,20')
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
# Now run the tensorboard command to view the visualizations (profile plugin).
</code></pre>
<h2 id="arguments">Arguments</h2>
<p>log_dir: the path of the directory where to save the log files to be
parsed by TensorBoard.
histogram_freq: frequency (in epochs) at which to compute activation and
weight histograms for the layers of the model. If set to 0, histograms
won't be computed. Validation data (or split) must be specified for
histogram visualizations.
write_graph: whether to visualize the graph in TensorBoard. The log file
can become quite large when write_graph is set to True.
write_images: whether to write model weights to visualize as image in
TensorBoard.
update_freq: <code>'batch'</code> or <code>'epoch'</code> or integer. When using <code>'batch'</code>,
writes the losses and metrics to TensorBoard after each batch. The same
applies for <code>'epoch'</code>. If using an integer, let's say <code>1000</code>, the
callback will write the metrics and losses to TensorBoard every 1000
batches. Note that writing too frequently to TensorBoard can slow down
your training.
profile_batch: Profile the batch(es) to sample compute characteristics.
profile_batch must be a non-negative integer or a tuple of integers.
A pair of positive integers signify a range of batches to profile.
By default, it will profile the second batch. Set profile_batch=0
to disable profiling.
embeddings_freq: frequency (in epochs) at which embedding layers will be
visualized. If set to 0, embeddings won't be visualized.
embeddings_metadata: a dictionary which maps layer name to a file name in
which metadata for this embedding layer is saved. See the
<a href="https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional">details</a>
about metadata files format. In case if the same metadata file is
used for all embedding layers, string can be passed.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If histogram_freq is set and no validation data is provided.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LRTensorBoard(tf.keras.callbacks.TensorBoard):
    def __init__(self, log_dir, **kwargs):  # add other arguments to __init__ if you need
        super().__init__(log_dir=log_dir, **kwargs)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs.update({&#39;lr&#39;: tf.keras.backend.eval(self.model.optimizer.lr)})
        super().on_epoch_end(epoch, logs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.callbacks.TensorBoard</li>
<li>tensorflow.python.keras.callbacks.Callback</li>
<li>tensorflow.python.keras.utils.version_utils.TensorBoardVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sequence_transformer.training_utils.LRTensorBoard.on_epoch_end"><code class="name flex">
<span>def <span class="ident">on_epoch_end</span></span>(<span>self, epoch, logs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs metrics and histogram summaries at epoch end.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_epoch_end(self, epoch, logs=None):
    logs = logs or {}
    logs.update({&#39;lr&#39;: tf.keras.backend.eval(self.model.optimizer.lr)})
    super().on_epoch_end(epoch, logs)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sequence_transformer" href="index.html">sequence_transformer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sequence_transformer.training_utils.BestModelSaverCallback" href="#sequence_transformer.training_utils.BestModelSaverCallback">BestModelSaverCallback</a></code></h4>
<ul class="">
<li><code><a title="sequence_transformer.training_utils.BestModelSaverCallback.on_epoch_end" href="#sequence_transformer.training_utils.BestModelSaverCallback.on_epoch_end">on_epoch_end</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sequence_transformer.training_utils.CustomExponentialDecayLR" href="#sequence_transformer.training_utils.CustomExponentialDecayLR">CustomExponentialDecayLR</a></code></h4>
<ul class="">
<li><code><a title="sequence_transformer.training_utils.CustomExponentialDecayLR.get_config" href="#sequence_transformer.training_utils.CustomExponentialDecayLR.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sequence_transformer.training_utils.CustomLRSchedule" href="#sequence_transformer.training_utils.CustomLRSchedule">CustomLRSchedule</a></code></h4>
<ul class="">
<li><code><a title="sequence_transformer.training_utils.CustomLRSchedule.get_config" href="#sequence_transformer.training_utils.CustomLRSchedule.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sequence_transformer.training_utils.LRTensorBoard" href="#sequence_transformer.training_utils.LRTensorBoard">LRTensorBoard</a></code></h4>
<ul class="">
<li><code><a title="sequence_transformer.training_utils.LRTensorBoard.on_epoch_end" href="#sequence_transformer.training_utils.LRTensorBoard.on_epoch_end">on_epoch_end</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>